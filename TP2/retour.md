# ğŸ“˜ ExpÃ©rimentation 2 â€” Optimisation des hyperparamÃ¨tres

Dans cette expÃ©rience, nous utilisons **le mÃªme jeu de donnÃ©es** pour optimiser les trois modÃ¨les  
(**Random Forest**, **AdaBoost**, **XGBoost**) via **GridSearchCV**.

Le paramÃ¨tre donnÃ© est :  
**test_size = 0.8**  
â†’ donc **80 % = test**, **20 % = train**

> Le dataset complet nâ€™a pas Ã©tÃ© explicitement fourni, mais les matrices de confusion permettent dâ€™infÃ©rer la taille du test.

La matrice de confusion du meilleur modÃ¨le XGBoost donne :  
- Test = 16 724 + 2 870 + 2 903 + 10 766 = **33 263 Ã©chantillons**

Donc le dataset total vaut environ :  
**Total â‰ˆ 33 263 / 0.8 = 41 579 Ã©chantillons**  
Train = Total â€“ Test = **8 316 Ã©chantillons**

---

# ğŸ“Š Table 4 â€” Taille du jeu de donnÃ©es (ExpÃ©rimentation 2)

| Jeu | Taille |
|-----|--------|
| **Train** | **â‰ˆ 8 316** |
| **Test** | **33 263** |

---

# âš™ï¸ 1. HyperparamÃ¨tres explorÃ©s + justification

## ğŸŒ² Random Forest
HyperparamÃ¨tres explorÃ©s :
```python
{
    "n_estimators": [100, 300],
    "max_depth": [None, 10, 20]
}
```
Justification :

n_estimators : augmenter le nombre dâ€™arbres amÃ©liore la stabilitÃ© mais augmente le temps de calcul.

max_depth : permet de contrÃ´ler lâ€™overfitting ; None = croissance libre de l'arbre.

## âš¡ AdaBoost

HyperparamÃ¨tres explorÃ©s :

```python
{
    "n_estimators": [50, 200],
    "learning_rate": [0.5, 1.0, 2.0]
}
```

Justification :

n_estimators : plus dâ€™itÃ©rations â†’ meilleur ajustement mais plus lent.

learning_rate : contrÃ´le lâ€™importance de chaque modÃ¨le faible (trade-off stabilitÃ© / prÃ©cision).

## ğŸš€ XGBoost

HyperparamÃ¨tres explorÃ©s :

```python
{
    "n_estimators": [100, 300],
    "max_depth": [3, 6, 10],
    "learning_rate": [0.05, 0.1, 0.2]
}
```

Justification :

n_estimators : robustesse contre variance.

max_depth : profondeur des arbres â†’ contrÃ´le du sur-apprentissage.

learning_rate : plus la valeur est faible, plus le modÃ¨le apprend â€œlentementâ€ mais finement.

# ğŸ“ 2. Nombre de plis utilisÃ©s

â†’ Dans le code :

cv = 3

# ğŸ§® 3. Nombre total dâ€™entraÃ®nements effectuÃ©s

ModÃ¨le	Combinaisons	CV (3)	Total entraÃ®nements
RandomForest	2 Ã— 3 = 6	Ã—3	18
AdaBoost	2 Ã— 3 = 6	Ã—3	18
XGBoost	2 Ã— 3 Ã— 3 = 18	Ã—3	54

# ğŸ“‘ 4. Tableau de rÃ©sultats (ExpÃ©rimentation 2)
## ğŸŒ² Random Forest
Random Forest	Train Accuracy	CPU time	Test Accuracy	HyperparamÃ¨tres
DÃ©faut	â€”	10.20 s	0.8173	n_estimators=100, max_depth=None
OptimisÃ©	â€”	71.18 s	0.8205	n_estimators=300, max_depth=20

â¡ï¸ L'amÃ©lioration reste lÃ©gÃ¨re mais rÃ©elle.

## âš¡ AdaBoost
AdaBoost	Train Accuracy	CPU time	Test Accuracy	HyperparamÃ¨tres
DÃ©faut	â€”	1.71 s	0.8032	n_estimators=50, learning_rate=1.0
OptimisÃ©	â€”	17.66 s	0.8092	n_estimators=200, learning_rate=1.0

â¡ï¸ Le gain est modÃ©rÃ©. Lâ€™augmentation du nombre dâ€™estimateurs amÃ©liore lÃ©gÃ¨rement les performances.

## ğŸš€ XGBoost
XGBoost	Train Accuracy	CPU time	Test Accuracy	HyperparamÃ¨tres
DÃ©faut	â€”	0.20 s	0.8256	n_estimators=100, max_depth=6, learning_rate=0.1
OptimisÃ©	â€”	11.60 s	0.8264	n_estimators=300, max_depth=6, learning_rate=0.1

â¡ï¸ TrÃ¨s lÃ©gÃ¨re amÃ©lioration ; XGBoost Ã©tait dÃ©jÃ  performant par dÃ©faut.

# ğŸ“ 5. Analyse par mÃ©thode
## ğŸŒ² Random Forest

Lâ€™augmentation du nombre dâ€™arbres et la limitation de la profondeur permettent un modÃ¨le lÃ©gÃ¨rement meilleur.

Gain faible car RF est assez robuste par dÃ©faut.

Temps de calcul trÃ¨s Ã©levÃ© lors du Grid Search.

## âš¡ AdaBoost

TrÃ¨s sensible au nombre dâ€™estimateurs.

Les gains restent faibles, modÃ¨le parfois limitÃ© par la nature des donnÃ©es.

Mais amÃ©lioration constante en augmentant n_estimators.

## ğŸš€ XGBoost

Meilleures performances globales, mÃªme avant optimisation.

Lâ€™optimisation ne change que trÃ¨s peu lâ€™accuracy : le modÃ¨le Ã©tait dÃ©jÃ  presque optimal.

Temps dâ€™entraÃ®nement trÃ¨s faible par rapport aux deux autres (implÃ©mentation optimisÃ©e).

# âœ… Conclusion gÃ©nÃ©rale

XGBoost est le meilleur modÃ¨le, mÃªme sans optimisation poussÃ©e.

Random Forest et AdaBoost progressent lÃ©gÃ¨rement aprÃ¨s tuning, mais sans franchir XGBoost.

Les grilles dâ€™hyperparamÃ¨tres restent raisonnables pour Ã©viter une explosion du temps de calcul tout en couvrant les valeurs les plus pertinentes.